{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Text Classification with CNN1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFWGIcMT6QvSfg9us7A//T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoDeMiro/Ember/blob/main/04_Text_Classification_with_CNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Ajánlott ovlasmányok\n",
        "\n",
        "<a href=\"http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/\">Understanding how CNN perform Text Classification with Word Embeddings</a>\n"
      ],
      "metadata": {
        "id": "bH-g9dVXecC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 - Szöveg osztályozás Konvolúciós Hálóval.\n",
        "\n",
        "A feladat, hogy a szövegeket (dokumentumokat) egy előre meghatározott számú halmaz valamelyik eleméhez rendeljük. Konvulúciós háló esetén az egyik problémát az jelenti, hogyha szöveg szavait vektorokká alakítjuk, akkor is különböző hosszú szövegek mátrixok álnak elő belőle.\n",
        "\n",
        "Amit itt bemutatok az a <a href=\"https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\">Hierarchical Attention Networks for Document Classification</a> cikk alapján előállított model. De sorra veszem a két másik eljárást a CNN-t és az LSTM-t is.\n"
      ],
      "metadata": {
        "id": "SdOaK6fUkWxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BwX23qOeZVng"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "# Erre most nincs szükségem de itt hagyom\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "metadata": {
        "id": "lvMy8rF-Zci4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WSZ8-J0wleo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Megtísztítom a szövegtörzset a sortörésektől vagy a html tagoktól, továbbá elvégzem a szükséges átalakításokat például minden char legyen lowercase, etc."
      ],
      "metadata": {
        "id": "H0IOtb3MlfsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"\\\\\", \"\", string)    \n",
        "    string = re.sub(r\"\\'\", \"\", string)    \n",
        "    string = re.sub(r\"\\\"\", \"\", string)    \n",
        "    return string.strip().lower()\n"
      ],
      "metadata": {
        "id": "fgRG3z2uZe4e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download imdb train and keep the files in the working directory\n",
        "!wget https://github.com/prateek22sri/Sentiment-analysis/raw/master/labeledTrainData.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0oSmr5madGI",
        "outputId": "ef91af14-3c4d-45cc-d8dc-2cb8a07bb553"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-25 08:50:31--  https://github.com/prateek22sri/Sentiment-analysis/raw/master/labeledTrainData.tsv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/prateek22sri/Sentiment-analysis/master/labeledTrainData.tsv [following]\n",
            "--2022-01-25 08:50:31--  https://raw.githubusercontent.com/prateek22sri/Sentiment-analysis/master/labeledTrainData.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33556378 (32M) [text/plain]\n",
            "Saving to: ‘labeledTrainData.tsv’\n",
            "\n",
            "labeledTrainData.ts 100%[===================>]  32.00M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-01-25 08:50:31 (302 MB/s) - ‘labeledTrainData.tsv’ saved [33556378/33556378]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download glove word vector\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_hTJ1pmaBtK",
        "outputId": "d0787429-fd8d-4932-d375-bb6a85700f6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-25 08:44:27--  https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /account/login?returnUrl=%2Fc%2Fword2vec-nlp-tutorial [following]\n",
            "--2022-01-25 08:44:27--  https://www.kaggle.com/account/login?returnUrl=%2Fc%2Fword2vec-nlp-tutorial\n",
            "Reusing existing connection to www.kaggle.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘labeledTrainData.tsv’\n",
            "\n",
            "labeledTrainData.ts     [ <=>                ]   6.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-25 08:44:27 (82.1 MB/s) - ‘labeledTrainData.tsv’ saved [6585]\n",
            "\n",
            "--2022-01-25 08:44:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-25 08:44:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-25 08:44:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.15MB/s    in 2m 45s  \n",
            "\n",
            "2022-01-25 08:47:13 (4.98 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
        "print(data_train.shape)\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(data_train.review.shape[0]):\n",
        "    text = BeautifulSoup(data_train.review[idx], \"lxml\")\n",
        "    texts.append(clean_str(text.get_text()))\n",
        "    labels.append(data_train.sentiment[idx])\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXiV2xIyZwiR",
        "outputId": "85ab25c1-bea0-4b79-f896-16c90f7410df"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('len(texts) = ', len(texts))\n",
        "print('len(labels) = ', len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKL-YPV6l_8v",
        "outputId": "eebca248-0f11-4414-c78f-a67c67b74e56"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(texts) =  25000\n",
            "len(labels) =  25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAX_NB_WORDS = ', MAX_NB_WORDS)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTcPcldScNdv",
        "outputId": "b0bc6b85-9c11-4630-c5f1-8d136306177c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX_NB_WORDS =  20000\n",
            "Found 81501 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('type(sequences) = ', type(sequences))\n",
        "print('len(sequences) = ', len(sequences))\n",
        "print('type(sequences[0]) = ', type(sequences[0]))\n",
        "print('len(sequences[0]) = ', len(sequences[0]))    # első dok/szöveg hossza\n",
        "print('len(sequences[1]) = ', len(sequences[1]))    # második dok/szöveg hossza\n",
        "print('len(sequences[2]) = ', len(sequences[2]))    # harmadik dok/szöveg hossza\n",
        "print('sequences[0][:5] = ', sequences[0][:5])      # első dok első 5 szava\n",
        "print('sequences[0][0] = ', sequences[0][0])           # első dok, első szó\n",
        "print('sequences[0][1] = ', sequences[0][1])           # első dok, második szó\n",
        "print('sequences[0][2] = ', sequences[0][2])           # első dok, harmadik szó\n",
        "print('type(word_index) = ', type(word_index))\n",
        "\n",
        "# First doc, first word\n",
        "print(list(word_index.keys())[list(word_index.values()).index(sequences[0][0])])\n",
        "print(list(word_index.keys())[list(word_index.values()).index(sequences[0][1])])\n",
        "print(list(word_index.keys())[list(word_index.values()).index(sequences[0][2])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6_FZV9FnAYZ",
        "outputId": "7c878075-d059-4e20-b976-45043523616f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(sequences) =  <class 'list'>\n",
            "len(sequences) =  25000\n",
            "type(sequences[0]) =  <class 'list'>\n",
            "len(sequences[0]) =  419\n",
            "len(sequences[1]) =  160\n",
            "len(sequences[2]) =  352\n",
            "sequences[0][:5] =  [15, 29, 10, 537, 165]\n",
            "sequences[0][0] =  15\n",
            "sequences[0][1] =  29\n",
            "sequences[0][2] =  10\n",
            "type(word_index) =  <class 'dict'>\n",
            "with\n",
            "all\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Korábban utaltam rá, hogy ha számokká alakított szövegeken olyan osztályozást szeretnénk végrehajtani amelynek az alapját a konvolúciós háló végzi, akkor szükséges az egyes osztályozásra/kategorizálásra váró szövegeket azon méretűre hozni.\n",
        "\n",
        "Hogy néz ki az azonos méretre hozás ilyen szövegek esetében. Itt a *sorok* száma a szavak száma az adott dokumentumban. Az *oszlopok* száma fix, ez a vektorrá alakított szavak hosszának felel meg, amit a programunk elején megadtunk. Ebben a példaprogram ezt az értéket a `EMBEDDING_DIM = 100` értékben határoztam meg.\n",
        "\n",
        "Fontos kitétel, hogy a szavak vektorrá alakítását nem én és most végeztem el, hanem egy széles körben használt angol nyelvű **Word2Vec** modelt használtam fel. Ugyanakkor fontos megérteni, hogy például egy másik kutatásban az így előállított alacsonyabb dimenziójú reprezentációt (word_embeding) és én állítottam el. Ennek a tanítása ugyanakkor időigényes, és nagy adathalmazt igényel. Mivel ennek a bemutatónak a célja a Konvolúció bemutatása a szövegosztályozásban ezért a **Word2Vec** transformációtól itt most eltenkintek.\n",
        "\n",
        "## Szükséges átalakítás a konvolúcióhoz\n",
        "\n",
        "Tehát, hogy osztályozni tudja a dokumentumokat azonos méretű mátrixokká kell alakítanom őket. Mive az oszlopok száma adott, csak a sorok száma változik, a sorok számát kell azonos értékre hoznom.\n",
        "\n",
        "Erre többféle módszer is van, most ebbe nem megyek bele."
      ],
      "metadata": {
        "id": "9KN_qtDcqmAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "4pIzbyDecU3N"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', one_hot_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64JQagnvcYUt",
        "outputId": "c330a57a-2521-4ae1-e6a6-b603db99ce93"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (25000, 1000)\n",
            "Shape of label tensor: (25000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYqcFd1ytO48",
        "outputId": "9d8c7fcf-6468-47bc-f2f0-b2a0e8c85a1b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkKUsRu6tqgk",
        "outputId": "1f843672-6f2f-4ccf-c20e-97607b882076"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labet to One-Hot-Vector\n",
        "\n",
        "Eredetileg két kategóriánk volt, ugyhogy az ebből létrehozott One-Hot-Vector is kettő hosszú lesz. Több kategória esetén értelemszerűen hosszabb lenne."
      ],
      "metadata": {
        "id": "wiOlI9BgtdmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels.count(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icLqLEYtw0ky",
        "outputId": "113a72e4-e6f0-437e-83bb-9face33b166f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12500"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def zero_rule_algorithm_classification(sample):\n",
        "  uniqueValues = np.unique(sample)\n",
        "  counts = []\n",
        "  for i in uniqueValues:\n",
        "    count = sample.count(i) / len(sample)\n",
        "    counts.append(count)\n",
        "  \n",
        "  return counts"
      ],
      "metadata": {
        "id": "MzB8DOmVu74c"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_rule_algorithm_classification(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWbraoL4x_5L",
        "outputId": "bd80ee1a-1e8e-41b4-b1b8-45dc1d470530"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Rule Classifier\n",
        "\n",
        "By the way, mindíg jó öltelt megnézni, hogy mi a kimeneit változó eloszlása. Ez egy referencia pont, hogy mihez képest kell majd a modellemnek jobban teljesítenie."
      ],
      "metadata": {
        "id": "Q6Q8flZJ1jsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "one_hot_labels = one_hot_labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
      ],
      "metadata": {
        "id": "jn-9v1i-caNk"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle the data\n",
        "\n",
        "Keverjük össze az adatokat, hogy véletlenszerűen válaszunk belőlük a tanuló és a teszt mintába is."
      ],
      "metadata": {
        "id": "OFq-1M7O2FWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n",
        "\n",
        "print('Number of positive and negative reviews in traing and validation set ')\n",
        "print(y_train.sum(axis=0))\n",
        "print(y_val.sum(axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtzPIQbMccw6",
        "outputId": "bf7099d2-a976-4a7f-9fc9-40ef67acee6b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive and negative reviews in traing and validation set \n",
            "[ 9973. 10027.]\n",
            "[2527. 2473.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_DIR = \"glove/glove.6B/\"\n",
        "GLOVE_DIR = \"\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROBB2qkecfLL",
        "outputId": "deb22d4c-cd0c-4dc5-b809-560619a267d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 400000 word vectors in Glove 6B 100d.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "4o8cHquBciF6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "id": "YNhAnxapc0CP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
        "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
        "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "l_flat = Flatten()(l_pool3)\n",
        "l_dense = Dense(128, activation='relu')(l_flat)\n",
        "preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"model fitting - simplified convolutional neural network\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbC6uOT4c2LI",
        "outputId": "1976dd16-f4ec-4c02-a117-0a3ba19190e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model fitting - simplified convolutional neural network\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1000)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 1000, 100)         8150200   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 996, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 199, 128)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 195, 128)          82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 39, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 35, 128)           82048     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 1, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,395,194\n",
            "Trainable params: 8,395,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "#           epochs=10, batch_size=128)\n",
        "\n",
        "# Ez nagyon sokáig tartana, úgyhogy most egy kicsit visszaveszek\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=1, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvtRMDoXc_SR",
        "outputId": "75d4ca9d-c2ba-4a95-8feb-bf4741e41497"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 246s 2s/step - loss: 0.6116 - acc: 0.6615 - val_loss: 0.4378 - val_acc: 0.8040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a3b4e210>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "id": "GIbGzD9HdGA-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying a more complex convolutional approach\n",
        "convs = []\n",
        "filter_sizes = [3,4,5]"
      ],
      "metadata": {
        "id": "NzzhzI5Pdvew"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)"
      ],
      "metadata": {
        "id": "Bw7xiiC3dy6J"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Conv1D()"
      ],
      "metadata": {
        "id": "wc47KjS7fAfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fsz in filter_sizes:\n",
        "    l_conv = Conv1D(128, fsz, activation='relu')(embedded_sequences)\n",
        "    l_pool = MaxPooling1D(5)(l_conv)\n",
        "    convs.append(l_pool)"
      ],
      "metadata": {
        "id": "yLZLNUxpd1Vq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_merge = Concatenate(axis=1)(convs)\n",
        "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
        "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
        "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "l_flat = Flatten()(l_pool2)\n",
        "l_dense = Dense(128, activation='relu')(l_flat)\n",
        "preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"model fitting - more complex convolutional neural network\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CnOBVuNd7m2",
        "outputId": "f3ab933c-8006-4f26-ab60-e23b8e310820"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model fitting - more complex convolutional neural network\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 1000)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1000, 100)    8150200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 998, 128)     38528       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 997, 128)     51328       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 996, 128)     64128       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 199, 128)    0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 199, 128)    0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 199, 128)    0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 597, 128)     0           ['max_pooling1d_3[0][0]',        \n",
            "                                                                  'max_pooling1d_4[0][0]',        \n",
            "                                                                  'max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 593, 128)     82048       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 118, 128)    0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 114, 128)     82048       ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 3, 128)      0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 384)          0           ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          49280       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            258         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,517,818\n",
            "Trainable params: 8,517,818\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "#           nb_epoch=20, batch_size=50)\n",
        "\n",
        "# Ez kicsit sokáig tartana, úgyhogy most egy kicsit visszaveszek\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=1, batch_size=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCS6jtSKd8E4",
        "outputId": "46474c3a-4259-4bd3-a352-c9050bcf9278"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400/400 [==============================] - 583s 1s/step - loss: 0.5372 - acc: 0.7102 - val_loss: 0.3295 - val_acc: 0.8564\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62a12b7450>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fo4gLaySgITD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}